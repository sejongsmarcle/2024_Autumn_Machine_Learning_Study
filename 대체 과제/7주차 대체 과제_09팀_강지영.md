1.  서로 다른 하이퍼파라미터나 알고리즘을 사용하거나 데이터 샘플링 방법 변경, 모델마다 다른 전처리 방법을 사용하면 좋은 결과를 얻는다.
2.  직접 투표 분류기는 횟수를 중요시하고 많은 투표를 얻은 클래스를 선택하고 간접 투표 분류기는 클래스의 평균 확률 추정값을 중 가장 높은 확률을 가진 클래스를 선택한다.
3.  배깅, 페이스팅, 랜덤 포레스트는 완전 병렬화가 가능하고 부스팅은 병렬화가 제한적이다.  스태킹은 1차 모델만 병렬화 가능하다.
4.  별도 검증 데이터 없이 배깅 과정에서 사용되지 않은 샘플로 모델 성능을 효율적으로 평가할 수 있다.
5.  임의의 분할점 선택과 전체 데이터 사용으로 랜덤성을 추가한다. 그리고 과적합을 줄이고 속도를 높여 랜덤 포레스트보다 빠르다.
6.  약한 학습기의 수를 늘리거나 학습률을 증가시켜 개선할 수 있다.
7.  그레이디언트 부스팅의 과대적합을 줄이려면 학습률을 낮춰야 한다.





